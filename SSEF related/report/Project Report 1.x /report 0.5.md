# Project Report

## Introduction

### Solar radiation[^24][^25]

When it comes to energy generation, solar power remains the most promising renewable energy source in Singapore. With an average solar irradiance of 1,580 kWh/(m^2^*year) and about 50% more solar radiation than temperate countries, solar photovoltaic (PV) generation has the greatest potential for wider deployment in Singapore.[^24] Moreover, a report by National Renewable Energy Lab [^25] has indicated that from 2010 to 2017 there has been approximately a 70% reduction in the total PV system hardware cost. Combining great potential and decreasing cost, there is a strong demand to deploy solar power widely in Singapore. Precise solar radiation prediction will be the key to successful integration of solar power.

### Available prediction tools[^4][^5]

In order to make accurate weather predictions, people have been coming up with all sorts of prediction methods. Most approaches are based on Numerical Weather Prediction (NWP) models, which solves differential equations to describe the dynamics in the atmosphere. Another type of prediction tools is based on statistical learning, which derives functional dependencies directly from the observations, also known as machine learning.

Currently, one of the most widely used weather prediction models based on NWP is known as Weather Research and Forecast (WRF) model. The model has been used in the weather forecast in the study[^20][^21] and has proved its reliability by its successful power output forecast in the context of Smart Grids and Renewable Energy[^22]. The WRF model uses the computing power to make a forecast of many atmospheric variables such as temperature, pressure, wind and rainfall. The core of the model is the mathematical models based on physical equations with all kinds of real-time data collected by different meteorological stations or weather satellites across the country as the input. By analyzing how the variables will interact with one another based on physical equations, the model will simulate the whole atmospheric environment and make predictions based on different requirements. For example, the WRF model can simulate the movement and thickness of the cloud and based on the real-time simulation, it can predict the solar potential of the region with very high accuracy because the environment simulated is very close to the actual world and they are based on the same physical principles.

The WRF model is commonly used to make short-term weather predictions and long-term climate predictions. One reason that restricts the WRF’s performance is the model requires very huge computing powers. Manipulating vast amounts of data and performing complex calculation requires some of the most powerful supercomputers in the world and it may still take huge amounts of time to complete the prediction.  Despite it is highly time consuming and requires numerous computing power, the WRF model requires huge amount of prior knowledge to interpret the computer forecast and because of this, very few people can operate the model to make the predictions as they want.

Due to the limitations of the WRF model, there is a strong demand to develop alternative methods to make the preidctions and that is where machine learning comes in. Machine Learning, that research different algorithms, can learn information from dataset and make a useful tool to predict the solar radiation with much shorter time taken.[^23] Essentially, various algorithms can accept a given set of training examples and derive a general trend over time.[^26] There are different machine learning algorithms, such as Support Vector Machine (SVM), Artificial Neural Network (ANN), K-Nearest Neighbours and Time series. The focus of the study is to learn about the algorithms and apply them into this certain problem. 

## Methodology

### Data pre-processing

The whole dataset provided consists of solar-related data at 62 different locations across Singapore generated by the WRF model for the time period 1 Jan 2018, 8 am to 1 Jan 2019, 8 am. There are four columns in each dataset: Time, Direct Solar Radiation (SWDIR), Diffuse Solar Radiation (SWDIF) and Gradient Level Wind (GLW) and only first three columns are interested in this study. When both SWDIR and SWDIF are zero, it corresponds to the night time when there is no solar radiation at the location. 

In this study, the solar potential is measured by clear-sky ratio (CSR), defined as 
$$
\text{CSR} = \frac{\text{SWDIR}}{\text{SWDIR}+\text{SWDIF}}
$$
where $\text{CSR}$ ranges from 0 to 1 by its definition. As Singapore has very limited land area, if there is no cloud cover, the total amount of radiation from the sun is rather constant across the country and hence CSR can be an applicable measure of solar potential across the country. In order to represent the solar radiation intuitively, the CSR value has been divided into ten continuous even groups (Group 1 to Group 10) from 0 to 1, namely $0.0-0.1, 0.1-0.2, 0.2-0.3 … 0.9 - 1.0$.

The CSR values at night, which are expected to be NaN (not a number) by its definition, are classified to Group 0, in order to maintain the continuity of the dataset. 

<!--In order to reduce the requirement of users, the Time indexes, given in the format of floats, have been converted to readable string-like floats. For example, the 2^nd^ time index is *0.254167*, which can be confusing as the meaning of the index remains unknown, has been converted to "*201801010815*", which can be understood intuitively. Due to the requirement of the Sci-kit package used in the study, the inputs are supposed to be in the format of floats, so the format is maintained in the processed dataset, but in a human-readable way.-->

For certain models, time series model in this case, the model requires specific datetime-type object input, By calculating the time interval of the data collection, a series of datetime-type object with known start, end and frequency is generated to fulfill the requirement of the model input.

The whole dataset has been normalized in the mean and standard deviation. 
$$
X_\text{norm(i, j)} = \frac{X_{(i, j)}-\text{mean}(X_j)}{\text{std}(X_j)}
$$
where $X_j$ is the $j^{\text{th}}$ column of matrix $X$. After normalising the dataset, the mean becomes zero and has unit variance along each feature and can be less sensitive to the scale of features.



## Various Models Used

Machine learning models are able to learn from data based on different algorithms. In supervised learning, computers are usually presented with sample input and their desired outputs and the goal is to learn a general function that maps inputs to outputs[^3]. There are generally two types of models in supervised learning: classification and regression. This section will explain different algorithms under the two categories.

### Classification models

The purpose of classification models is to identify which of a set of categories the new observation belongs to, on the bias of a training set of data containing observations whose catrgory membership is known.[^1] In this case, the new solar radiation data is categorized to one of the ten groups between the interval 0-1 based on its location and time data. Two classification models, Support Vector Classifier and Artificial Neural Network, and their algorithms are explained in this section.

#### Support Vector Classifier[^9][^10][^11][^12][^13]

Support Vector Machine algorithms (SVMs) are a set of supervised learning methods used for classification, regression and outlier detection. The objective of the algorithm is to find a hyperplane in an $N$-dimensional space ($N$ - the number of features) that distinctly classifies the data points.

![image-20181218223035322](../Downloads/MachineLearning2018ESTaR/SSEF%20related/image-20181218223035322-5143435.png)

As the figure above on the left suggests, there can be many possible hyperplanes available and the one with the maximum margin is to be found, i.e the maximum distance between data points of both classes. Maximizing the margin distance reinforces the model and makes the future classification more reliable.

 ![image-20181218222925077](../Downloads/MachineLearning2018ESTaR/SSEF%20related/image-20181218222925077.png)

Hyperplanes refer to decision boundaries that contribute to classify the data points. Data points falling on either side of the hyperplane can be attributes to corresponding classes. The dimension of the hyperplane is equal to $N-1$, where $N$ is the number of features. Support vectors are data points that are closer the hyperplane and can influence the position and orientation of the hyperplane. With these support vectors, the margin of the classifier can be maximised.

Given training vectors $x_i \in \mathbb{R}^p, i=1,2,...,n$, in two classes and a vector $y \in \{1, -1\}^n$, SVC solves the primal problem: 
$$
\begin{align}\begin{aligned}\min_ {w, b, \zeta} \frac{1}{2} w^T w + C \sum_{i=1}^{n} \zeta_i\\\begin{split}\textrm {subject to } & y_i (w^T \phi (x_i) + b) \geq 1 - \zeta_i,\\
& \zeta_i \geq 0, i=1, ..., n\end{split}\end{aligned}\end{align}
$$
The dual is:
$$
\begin{align}\begin{aligned}\min_{\alpha} \frac{1}{2} \alpha^T Q \alpha - e^T \alpha\\\begin{split}
\textrm {subject to } & y^T \alpha = 0\\
& 0 \leq \alpha_i \leq C, i=1, ..., n\end{split}\end{aligned}\end{align}
$$
where $e$ is the vector of all ones, $C>0$ is the upper bound, $Q$ is an $n * n$ positive semidefinite matrix, $Q_{ij}≡y_iy_jK(x_i,x_j)$, where $K(x_i,x_j)=ϕ(x_i)^Tϕ(x_j)$ is the kernel. Here training vectors are implicitly mapped into a higher (maybe infinite) dimensional space by the function $ϕ$.

The decision function of SVC is:
$$
\operatorname{sgn}(\sum_{i=1}^n y_i \alpha_i K(x_i, x) + \rho)
$$

#### Artificial Neural Network[^6][^7][^8]

Artificial Neural Network, also Multi-layer Perceptron (MLP), is a supervised learning algorithm that can learn a non-linear function approximator for classification or regression with a set of features $X=x_1, x_2, ..., x_n$ and a target $y$. Between the input layer and the output layer, there can be one or more non-linear layers, called hidden layers. Because the structure of the algorithm is very similar to the brain, the algorithm is known as Artificial Neural Network. The figure 1 below shows a one-hidden layer MLP with scalar output.

![image-20181218222534024](../Downloads/MachineLearning2018ESTaR/SSEF%20related/image-20181218222534024-5143134.png)

The leftmost layer, known as the input layer, consists of a set of neurons {$X_i|x_1, x_2,...,x_n$} representing different input features. Each neuron in the hidden layer transforms the values from the previous layer with a weighted linear summation $w_1x_1 + w_2x_2 + ... + w_mx_m$, followed by a non-linear activation function $g(\cdot):R \rightarrow R$ - like the hyperbolic tan function. The output layer receives the values from the last hidden layer and transforms them into output values.

Different solvers, such as Stochastic Gradient Descent (SGD), Adam, or L-BFGS, can be used for MLP to train and the process is called Backpropogation. SGD is employed in this study as it is capable of handling large datasets (with thousands of training examples or more). The solver updates parameters using the gradient of the loss function with respect to a parameter that needs adaptation, i.e.
$$
w \leftarrow w - \eta (\alpha \frac{\partial R(w)}{\partial w} + \frac{\partial Loss}{\partial w})
$$
where $\eta$ is the learning rate which controls the step-size in the parameter space search. $Loss$ is the Cross-Entropy loss function used for the network. Backpropogation allows the information to go back from the cost backward through the network in order to compute the gradient and that means looping over the nodes starting at the final node in reverse topological order to compute the derivative of the final node output with respect to each edge's node tail. In this way, it will help us find out which part is responsible for the largest error and hence the parameters at that step can be updated to fit the model into the training set. 

### Regression Models

Regression process is usually used to estimate the relationship between a dependent variable and one or more independent variables. More specifically, it can help people understand how the dependent variable changes when any one of the independent variable is varied, while the other independent variables remain fixed.[^2] In this study,  three different regression models are used and the section will explain the algorithms behind each model.

#### K-Nearest Neighbours Regressor[^19]

The K-nearest neighbour regressor (KNN) is an algorithm that stores all available cases and make the numerical prediction based on a similarity measure, e.g. distance function. KNN identifies the K-nearest neighbours by some distance metric and then average  the value of these neighbours. The prediction is given by:
$$
\hat{Y} = \frac{1}{K} \sum_{i \in N_0}y_i
$$
Commonly for continuous variables, Euclidean distance given by:
$$
D = \sqrt{\sum_{i=1}^k (X_i - y_i)^2}
$$
Usually, it can be very helpful to give higher weights to the nearby points compared with faraway points and this can be achieved by assigning weights propotional to the inverse of the distance. By using different weights parameters, different effects can be achieved as shown in the dioagram below.

![image-20181219023833877](../Downloads/MachineLearning2018ESTaR/SSEF%20related/image-20181219023833877-5158313.png) 

By assign "distance" to weights parameter, the regressor can better fit the training data. However, it can also cause the problem of over-fitting and it is advisable to think twice before changing the weights parameter.

#### <!--Random Forest Regressor[^4][^5]-->

<!--Random Forest algorithm usually includes three parts: Decision Tree, Feature Bagging and Ensemble of the result.-->

<!--In the formation of a Decision Tree, the subsets of training data are randomly sampled with replacement and the target variable is fitted to regression models using each of the independent variables. The data is split at several split points for each independent variable. At each split point, the Sum of Squared Errors (SSE) between the predicted values and the actual values is calculated and compared against one another. The variable or point yielding the lowest SSE is hence chosen as the split point, also called root node. The process, known as "bagging" method, is recursively continue. Many regression Decision Trees are formed with several nodes in each tree and the ensemble of these decision trees is known as the "random forest".-->

<!--The Random Forest method introduces more randomness and diversity by applying the bagging method to the features. Rather than search for the best predictors greedily to find new split points, it randomly samples elements from the predictors, thus increasing the diversity and reducing the variance of the trees while keeping bias high or even increasing. The process is known as “Feature Bagging” and can help make the model more robust.-->

<!--In the prediction process, each new data point goes from the root node to the bottom until it is fitted to a function with all the different trees in the ensemble and the average prediction of each tree is used as the final result. The ensemble learning allows multiple algorithms to obtain better predictive preformance than that from any constitutent learning algorithm alone. By combing result of all the "trees" in the "forest" with ensemble learning, the more accurate prediction can be obtained.-->

#### Time-series model[^14][^15]

 An important aspect of time-series model is the concept of stationary processes. That means the autocorrelation and expectation do not vary with time. Known the solar radiation has its seasonal variation pattern, the time-series can be employed to make the prediction. The model is known as Seasonal Auto-Regressive Integrated Moving Average with eXogenous regressors model (SARIMAX) and is very simialr to a combination of the Auto-regressive model (AR), the Moving Average model (MA) and additional seasonal terms.

The auto-regressive (AR) process is a regression term that the explanatory variable depends on historical values. The formula is given below:
$$
X_t-\sum_{i=1}^q ϕ_iX_{t-i} = Z_t
$$
In the equation, $q$ is chosen depending on the autocorrelation functions and an AR(1)-model correspond to $q=1$. In this context, $Z_t \sim WN(0, \sigma^2)$.

The Moving Average (MA) process is another regression form where the prediction parameter depends on external shocks rather than historical data. Formally, the process is defined as:
$$
X_t = \sum_{i=0}^p \theta_iZ_{t-i}
$$
Here, $Z_i \sim WN(0, \sigma^2)$ and $Z_i$ are independent for all $i$ and for $i=0$, $\theta_0=1$.

The seasonal terms make sense when the time series have a clearly seasonal pattern. The $ARIMA(p,d,q)(P,Q,D)_s$ is given by:
$$
\phi^p(B)\Phi^P(B^s)(|1-B)^d(1-B^s)^DX_t=\theta^q(B)\Theta^Q(B^s)Z_t
$$
where $\Phi^P(B^s) = 1-\Phi B^s-...-\Phi^PB^s$ and $\Theta^P(B^s)$ follows analogously. The $P$, $D$ and $Q$ have the same properties as the $p$, $d$ and $q$, which namely represent AR parameters, differences, and MA parameters, however with a seasonal step. Thus $D=1$ gives the seasonal difference for some seasonality $s$: $(1-B)^{1*s}X_t = X_t - X_{t-s}$.

## Evaluation of results[^16][^17]

### Mean Absolute Error

A common performance metric is the Mean Absolute Error (MAE), given by:
$$
MAE = \frac{\sum_{i=1}^n|y_i-\hat{y}_i|}{n} = \frac{\sum_{i=1}^n|e_i|}{n}
$$
where $y_i$ and $\hat{y}_i$ corresponds to the actual value of $y$ and predicted value of $y$ and $e_i$ corresponds to the error between the two $y$ terms. As the MAE sums up the absolute errors, it can reflect the overall error from the actual trend where all individual terms have equal weights.

### Mean Squared Error

Another common performance metric is the Mean Squared Error (MSE), given by:
$$
MSE = \frac{\sum_{i=1}^n (y_i - \hat{y}_i)^2}{n}
$$
Same as the MAE, the terms in the formula represent the exact same meanings. By summing up the squared errors, MSE gives the outliers a higher weight and can reflect the variation of each individual term.

### Root Mean Squared Error

Similar to the MSE, Root Mean Squared Error (RMSE) gives higher weight to outliers and is given by:
$$
RMSE = \sqrt{\frac{\sum_{i=1}^n (y_i - \hat{y}_i)^2}{n}}
$$
By finding the square root of MSE, the RMSE can be directly interpreted in terms of measurement units, and hence is a better measure of accuracy than a correlation coefficient or MSE. While giving higher weights to terms with large errors, which are particularly not desired outcomes, RMSE has a tendency to be increasingly larger than MAE as the sample size increases. This can be problematic and restricts the application of RMSE in this study given the total dataset which has the size of nearly 10^7​^.

### Table of metrics and Analysis

| Model Name                          | Mean Absolute Error (MAE) | MAE percentage ($\%$) | Mean Squared Error (MSE) | Root Mean Squared Error (RMSE) | RMSE percentage ($\%$) |
| ----------------------------------- | ------------------------- | --------------------- | ------------------------ | ------------------------------ | ---------------------- |
| Support Vector Classifier (SVC)     | 2.52872                   | 25.29                 | 20.19782                 | 4.49420                        | 44.94                  |
| Multiple-layer Perceptron (MLP)     | 2.38804                   | 23.88                 | 18.77087                 | 4.33254                        | 43.33                  |
| K-Nearest Neighbour Regressor (KNN) | 0.92841                   | 9.28                  | 3.21602                  | 1.79333                        | 17.93                  |
| Time-series (SARIMAX)               | 0.82951                   | 8.30                  | 3.74552                  | 1.93533                        | 19.35                  |

From the table above, we can clearly see that the SARIMAX and KNN models have significantly higher performance than SVC and MLP models with the correspoding error metrics below half of the latter. This could be because the regression model makes use of the continuity of the training dataset. Compared classifying the solar radiation data into different groups based on the given time index and other variables, the regression model can better reflect the relationship between features $X$ and the solar radiation data $y$ and thus can make much more accurate predictions than the classification. The poor formance of the classification models can also be caused by inappropriate selection of features and this area can be further explored.

In between two regression models, the SARIMAX model has very similar performance as the KNN model, around 1% lower in MAE and around 2% higher in RMSE. This could be the result of making use of the seasonal trend. The solar radiation data can be influenced by change of the relative locations between the sun and the earth as well as other factors. Realising the seasonal trend and making use of the autocorrelation and expectation of the time series can contribute to reduce the overall difference between prediction and true values, thus reducing the MAE value. However, if we zoom in to all the individual predictions, large difference can exist between some predictions and corresponding  actual radiation data and hence leads to higher RMSE of the SARIMAX than the KNN.



## Conclusion and outlook

<!--I think there is not much to plan for the conclusion. For the future work part, I have prepared a few essays (while finding the similar essays) as the future work. The range of future work includes: try deep belief network, comparison regarding more aspects not just restricted accuracy but running time etc, (neural network model)-->

This study has explored different machine learning algorithms in order to find quick alternative methods to make solar radiation prediction rather than the currently most popular Weather Research and Forecast models. Several popular machine learning techniques such as Support Vector Machine, Artificial Neural Network, K_Nearest Neighbours and Time-series, have been studied and their performance in making the prediction has been evaluated based on different metrics including Mean Absolute Error and Root Mean Squared Error. The greatest achievement of the study is to build two models that are capable of completing the whole process including learning and prediction within only one or two days with relatively high accuracy. However, it is also worth noting that the classification models have very poor performance possibly due to inappropriate selection of features and this can be further studied. Moreover, more machine laerning techniques such as ensemble learning, which can combine two models to be a model with better performance, can be explored adn applied to the study and that will be the future work of the research.



## Bibliography

[^1]: https://en.wikipedia.org/wiki/Statistical_classification
[^2]: https://en.wikipedia.org/wiki/Regression_analysis 
[^3]: R.H. Inman, H.T.C. Pedro, C.F.M. Coimbra, Solar forecasting methods for renewable energy integration, Prog. Energy Combust. Sci. 39 (2013) 535–576. doi:10.1016/j.pecs.2013.06.002.
[^4]: Ernst, B., Oakleaf, B., Ahlstrom, M., Lange, M., Moehrlen, C., Lange, B., Focken, U.,Rohrig, K. Predicting the Wind. In Power and Energy Magazine, 5(6), 78-89, 2007.
[^5]: Nils Andre Treiber, Stephan Spath, Justin Heinermann, Lueder von Bremen and Oliver Kramer. Comparison of Numerical Models and Statistical Learning for Wind Speed Prediction.
[^6]: https://scikit-learn.org/stable/modules/neural_networks_supervised.html 
[^7]: https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier
[^8]: https://towardsdatascience.com/coding-neural-network-forward-propagation-and-backpropagtion-ccf8cf369f76
[^9]: https://towardsdatascience.com/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47
[^10]: https://scikit-learn.org/stable/modules/svm.html
[^11]: https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC
[^12]: I. Guyon, B. Boser, V. Vapnik - Advances in neural information processing 1993.[ “Automatic Capacity Tuning of Very Large VC-dimension Classifiers”](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.17.7215), 
[^13]: C. Cortes, V. Vapnik - Machine Learning, 20, 273-297 (1995). [“Support-vector networks”](https://link.springer.com/article/10.1007%2FBF00994018)
[^14]: Peter J Brockwell. Introduction to Time Series and Forecasting. Springer Texts in Statistics. Springer, 3rd ed. 2016.. edition, 2016. 
[^15]: https://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.html
[^16]: https://medium.com/human-in-a-machine-world/mae-and-rmse-which-metric-is-better-e60ac3bde13d
[^17]: https://www.vernier.com/til/1014/
[^18]: https://scikit-learn.org/stable/modules/neighbors.html#regression
[^19]: https://www.saedsayad.com/k_nearest_neighbors_reg.htm
[^20]: T.M.Giannaros, V.Kotroni, and K.Lagouvardos. Predicting lightning activity in Greece with the weather research and forecasting (wrf) model. Atmospheric Research, vol. 156. 1 - 13, 2015.
[^21]: Terren-Serrano, Guillermo. "Machine Learning Approach to Forecast Global Solar Radiation Time Series."(2016).[ http://digitalrepository.unm.edu/ece_etds/249] (Stephan Rasp and Sebastian Lerch. Neural networks for post-processing ensemble weather forecasts. arXiv: 1805.09091v1 [star.ML] 23 May 2018)
[^22]: Haupt, Sue & Kosovic, Branko. (2015). Big Data and Machine Learning for Applied Weather Forecasts Forecasting Solar Power for Utility Operations. 10.1109/SSCI.2015.79.  
[^23]: Aoife M.Foleyabd, Paul G.Leahyab, AntoninoMarvugliac, Eamon J.McKeoghabCurrent methods and advances in forecasting of wind power generation 
[^24]: https://www.ema.gov.sg/solar_photovoltaic_systems.aspx (accessed on 20 Nov 2018) 
[^25]: Ran Fu, David Feldman, Robert Margolis, Mike Woodhouse, and Kristen Ardani. U.S. solar photovoltaic system cost benchmark: Q1 2017. Sep.
[^26]: Aoife M.Foleyabd, Paul G.Leahyab, AntoninoMarvugliac, Eamon J.McKeoghabCurrent methods and advances in forecasting of wind power generation 
[^27]:  